{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np         # dealing with arrays\n",
    "from tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion\n",
    "\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "TRAIN_DIR = '../Dog-vs-Cat-Tensorflow/datasets/train/'\n",
    "TEST_DIR = '../Dog-vs-Cat-Tensorflow/datasets/test/'\n",
    "IMG_SIZE = 64\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(word_label):\n",
    "    \"\"\" Create an one-hot encoded vector from image name \"\"\"\n",
    "    if \"dog\" in word_label:\n",
    "        return np.array([1, 0])\n",
    "    elif \"cat\" in word_label:\n",
    "        return np.array([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DogVsCat(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DogVsCat, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3,64,stride=1,kernel_size=3),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64,64,stride=1,kernel_size=3),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64,128,stride=1, kernel_size=3),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.dropout = nn.Dropout()\n",
    "        # each convolution layer followed by MaxPooling2d decreases the size of the image\n",
    "        # the last size of the tensor coming out of sonv3 layer will be [batch size, 128 (channels), 6, 6]\n",
    "        self.fc1 = nn.Linear(128*6*6, 64) # so input size will be 128x6x6\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten the tensor\n",
    "        x = F.relu(self.fc1(x))  # use Fully connected layer 1\n",
    "        x = self.dropout(x)   # we usually use dropout after Fully connected layers\n",
    "        x = self.fc2(x)  # we didn't pass the x through fc2, earlier it was self.fc2()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DogVsCat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DogVsCat(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc1): Linear(in_features=4608, out_features=64, bias=True)\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (1): Softmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = sum([len(files) for r, d, files in os.walk(TRAIN_DIR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriveData(Dataset):\n",
    "\n",
    "    def __init__(self, path,transform=None):\n",
    "        self.Path = path\n",
    "        self.PathFIle =  [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    # Override to give PyTorch access to any image on the dataset\n",
    "    def __getitem__(self, index):\n",
    "        path = self.PathFIle[index]\n",
    "        path = os.path.join(self.Path,path)\n",
    "        # Always use one image processing package, so I removed PIL\n",
    "        # cv2 loads image in Blue, Green Red (BGR) format; convert it to RGB format\n",
    "        image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "        img_data = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        # torch (by default) requires images in the form -> [channels, width, height], but cv2 gives in form -> [width, height, channel]\n",
    "        # convert image to channel first\n",
    "        img_data = np.rollaxis(img_data, 2, 0).astype('float32')\n",
    "        image  = torch.from_numpy(img_data)\n",
    "        label  = torch.from_numpy(create_label(path))\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        cpt = sum([len(files) for r, d, files in os.walk(self.Path)])\n",
    "        return cpt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(create_label(\"train\\dog.9552.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"dog\" in \"train\\dog.9552.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = DriveData(path=TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dset_train, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x22508f18048>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [185/1563], Loss: 0.8758:  12%| | 185/1563 [03:01<22:33,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 1 # for demonstration purpose I have kept num epochs as 1. For full training make it 10 or 20 or even more if you want to see performance\n",
    "total_step = len(train_loader)\n",
    "train_loader = tqdm(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # CrossEntropy takes target indices not the one hot vector.\n",
    "        # values, indices = torch.max(labels)\n",
    "        _, targets = torch.max(labels, -1)\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Forward pass - We will be working on complete batch of 16 images. And not with 1 image at a time\n",
    "        # remember the images tensor is of shape -> [16, 3, 64, 64],\n",
    "        # labels tensor is of shape -> [16, 2] and targets tensor is of shape -> [16,]\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \"\"\"        for i in range(len(images)):\n",
    "            outputs = model(images[i].unsqueeze_(0))\n",
    "            loss = criterion(outputs[i], labels[i])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\"\"\"\n",
    "        \n",
    "        #if (i+1) % 100 == 0:\n",
    "        train_loader.set_description('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stopped the training in the middle as due to low compute power of my machine it will take hours. \n",
    "\n",
    "Try to correct the below code you wrote before with by looking at changes I made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(X_train)):\n",
    "        # get the inputs\n",
    "        label = torch.from_numpy(y_train[i])\n",
    "        inputs = torch.from_numpy(X_train[i])\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 1, 3, 3)\n",
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
